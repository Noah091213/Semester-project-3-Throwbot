\chapter{Discussion}
\label{chap:discussion}

\section{Machine Vision}
\subsection{Hough Transform}
Instead of \texttt{cv::HoughCircles}, other methods like searching for specific patterns within the defined target or looking at the colors of the target were considered. However, since the target and many other objects within the scene are circular, the Hough transform is both more versatile and more robust. Additionally, it is very easy to implement with OpenCV compared to any specific algorithm hardcoded to this scenario.

\subsection{Computing Homography}
Another, perhaps more robust, way of selecting points to compute the homography would be to find the circular holes in the table using \texttt{Vision::findCircularObject} and use that as the points for the homography as they are also known positions and would be more precise than having the user click on the pixels that correspond to the table corners. However, this was not implemented, as the original solution worked as intended.

\section{Trajectory Planning}
Trajectory generation for the ball and robot involves various methods with distinct trade-offs. For this project, methods were selected based on their impact on the primary goal. Because trajectory planning involves many interdependent parameters, it functions as an optimization problem where a "perfect" solution is elusive. Consequently, the approach to selecting variables and calculation methods varied; some were chosen through rigorous testing, while others relied on hypotheses and limited validation.

\subsection{Ball Trajectory}
The ball’s trajectory was optimized to meet specific project constraints. To qualify as a "throw," the object must travel horizontally with an upward trajectory at release. Thus, calculations enforce a minimum pitch of 22.5° ($\pi/8$ rad). This constraint is vital because the secondary objective—minimizing velocity—would otherwise favor a negative or near-zero pitch, given that the release point is higher than the target. Lower velocity is preferred as it reduces the required acceleration space, limiting the robot's range of motion and ensuring it stays within physical boundaries.

\subsection{Robot Path Planning}
As the joint limits are more restricting than the Cartesian boundaries, planning in joint-space seems optimal. While joint-space planning offers total control over joint positions, it complicates generating specific Cartesian movements. The primary objective is ensuring the Tool Center Point (TCP) reaches the release point with the correct velocity and direction. To simplify this, the path is split into two section:

\textbf{Lead-up Trajectory}: This phase aims to reach the release point at the target speed while respecting joint limits. By calculating this in joint space, the range of motion is optimized based on acceleration capabilities. However, the precise Cartesian velocity and direction are only guaranteed at the exact moment of release; prior points may have arbitrary vectors.

\textbf{Follow-through}: To account for mechanical release timing errors (e.g., the ball not losing contact instantly), the TCP continues along the ball’s trajectory for a set duration. This phase is calculated in Cartesian space to enforce a linear direction. Simultaneously, the robot decelerates to remain within safety limits. This deceleration also provides a buffer: if the ball is released late, its proximity to the target is compensated for by a lower release velocity, maintaining some degree of accuracy.

\subsection{Robot-Table Calibration}
The initial calibration produced RMS errors ranging from 0.1991 m to 1.3078 m, which is significantly higher than expected for a robot–table calibration. These large errors indicate that the estimated transformation was strongly affected by inconsistent data. Inspection of the 3D calibration plot showed that several points deviated clearly from the rest of the data, making them outliers.

While small misalignments between the calibration tool and the table surface could result in minor TCP deviations, they are unlikely to cause these outliers. The most likely source of the large inaccuracies is user measurement error. 

To improve robustness, four outliers showing disproportionate error margins were removed from the data set to prevent bias. After removing the outliers, the calibration was repeated, resulting in RMS errors between 0.0005 m and 0.0012 m. The final transformation is therefore considered substantially more reliable and suitable for the project.

\subsection{MATLAB Communication}
In the final program MATLAB Engine was used, but other options were explored. Alternatives include but are not limited to MATLAB MEX functions and CSV communication. CSV is the crudest version of this communication, as it required MATLAB to already be running and checking the repeatedly CSV file for changes. The code did work, but having two programs running defeated the point of integrating MATLAB to have one seamless system, so it was decided to move away from CSV. Some of the code was repurposed for the storing of data, as mentioned in chapter \ref{chap:datacollection}.

The other options explored are native to MATLAB. MEX is a way of recompiling a MATLAB script into something that can be directly integrated into, among others, C++ \cite{MEXwiki}. It works by compiling both the script and the C++ code in MATLAB and running it in that environment. That means that data can be passed back and forth directly in memory and would in theory be faster than MATLAB engine. Downsides do however include limitations on toolboxes that were needed, and the fact that running it in MATLAB could introduce more problems with communication with the robot and camera. 

MATLAB engine works by starting the script as a process in the background, passing the data by value through the API, running it through the script, and returning it by value through the API to the C++ code \cite{Enginewiki}. This is in theory slower, and with more data than needed for this project, it could be an issue having multiple copies of the same data. However, using it allowed running the C++ code natively instead of in MATLAB and was easier to compile with libraries. Additionally, it allowed the use of toolboxes like the Robotics System Toolbox, while still integrating MATLAB directly with the C++ code, and therefore MATLAB engine was the preferred option in this project.

\section{Results}
The results from the testing phase gives an indication as to how each parameter impacts the system. Initial testing was done by simulating throws using varied parameters to find a suitable setup for the real-world testing.
 
\subsection{Weighted Jacobian}
Using a weighted Jacobian forces some joints to be more active. The implementation of this was done to limit the use of Wrist 2, which is highly restricted by the safety limits. This causes an increased usage of the other joints which resulted in the joint limits being exceeded more often during the lead-up path. However, it was determined that the increase in non-clipped follow-through paths out-ways the negative impact during lead-up, and thus the weighted Jacobian was used for all further testing.

\subsection{Acceleration}
It is natural that the robot cannot accelerate at an infinite rate, but finding which acceleration is possible with limited error margins proved to be difficult. Testing the impact of acceleration on the kinematic model was the first step to find a suitable level. It was found that an acceleration of $5 rad/s^2$ seemed to be a good middle ground, as it allows for hits at almost all tested target points. Increasing the acceleration by 20\% to $6 rad/s^2$ only added one additional reachable target. Keeping the acceleration relatively low is desirable as to keep the robot from having to do aggressive movements. 

As the robot path planning uses a static acceleration for the lead-up sequence, it is imperative that the robot is able to hit this acceleration profile as well. The movement control of the robot during the throw sequence is done by providing target speeds to the joints at each time step. The acceleration allowed to hit those targets must also be specified. Naturally, an acceleration lower than that of the model means it is impossible for the robot to hit the target speed at any point. Additionally, using an acceleration much higher than the model may lead to the robot stuttering as it will only need to accelerate at the start of each time step. A balance was found at a robot acceleration 25\% higher than the model, this provided the lowest mean absolute velocity error, meaning the robot's actual velocity was the closest to the model's.

\subsection{Release Timing}
It was quickly noted that there is a delay between calling the gripper to release the ball, and the ball physically being free from the robot. Determining this delay precisely was nearly impossible without any sensor feedback showing when contact with the ball was lost. Two approaches was used to estimate the delay, first, the gripper and terminal output was filmed in slow motion at 480 frames per second, and secondly, testing was done using varied offsets. Using the slow motion video it was estimated that the delay was 20-25 frames translating to 40-50ms, which was backed by the testing which showed the best results at an offset of 48ms. 

\subsection{Velocity Profile}
The method of splitting the path in two sections leads to a velocity profile in the shape of a triangle as seen in figure \ref{fig:VelocityOverTime}. In theory the path could have been split in a much larger number of sections, which would allow for more complex velocity profiles, but would also increase the complexity of the kinematics and mathematics needed substantially. The theorized benefit of using more sections is to allow for a differentiated acceleration throughout the sequence. This could help minimize the effects of jerk experienced as each joint starts moving. Had the time frame allowed, it would have been interesting to further test this hypothesis and try different profiles. 

\subsection{Accuracy}
The accuracy shown by the final system does not fully meet the expectations defined in section \ref{sec:ProblemDefinition}. The best results achieved by the system, seen in table \ref{tab:test_throw}, managed only a 68\% success rate compared to the desired 90\% consistency. Of the remaining 32\% of throws in that test, 3 of 8 were boundary hits, meaning they were very close to being within the tolerances. Increasing the consistency likely comes down to tweaking some of the parameters discussed in the earlier sections. 