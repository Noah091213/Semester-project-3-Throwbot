\chapter{Discussion}
\label{chap:discussion}

\section{Machine Vision}
\subsection{Hough Transform}
Instead of \texttt{cv::HoughCircles}, other methods like searching for specific patterns within the defined target or looking at the colors of the target were considered. However, since the target and many other objects within the scene are circular means that the Hough transform is both more versatile and more robust, together with it being very easy to implement with OpenCV compared to any specific algorithm hardcoded to this scenario.

\subsection{Computing Homography}
Another, perhaps more robust, way of selecting points to compute the homography would be to find the circular holes in the table using \texttt{Vision::findCircularObject} and use that as the points for the homography as they are also known positions and would be more precise than having the user click on the pixels that correspond to the table corners. Though this was not implemented as the solution that was first made worked well enough.

\section{Trajectory Planning}
Generating a trajectory for the ball and subsequently a path for the robot can be done in a multitude of ways, each with pros and cons. Therefore, one should consider what the objective is and define the problem in detail before choosing a method. The methods chosen for this project were all decided based on their expected impact on the project goal. 

As the complete trajectory planning includes a substantial amount of parameters that can be tweaked endlessly, it essentially becomes an optimization problem. Since each parameter change will affect how all other parameters act, finding the perfect solution for any throw is almost impossible. Due to this fact, the approach when choosing static variables or calculation methods, differed based on the expected impact of that variable. For some calculations rigorous testing was done to find the impact of change in those variables, while for others simple hypothesis and limited testing was applied to make the decision.

\subsection{Ball Trajectory}
The ball's trajectory can be optimized to achieve different goals. The problem definition dictated the decisions as to how these calculations were done. In the spirit of the project, \emph{throwing} an object was defined such that the object must travel some distance horizontally and must have an upwards trajectory at the release point. In order to achieve this, the ball trajectory calculation enforces a minimum pitch of 22.5 degrees ($\pi /8 \ radians$). This enforcement is important, because the secondary goal of the calculation is to find the trajectory which requires the lowest necessary velocity. A low velocity is helpful when determining the robots path, as less space is needed to accelerate, thus limiting the range of motion and increasing the chance of staying within limits. 

However, since the TCP cannot get near the table, where the target is placed, all throws will have a release point higher than the target point. The lowest velocity necessary for such a throw is likely found with a negative or near 0 pitch, which does not satisfy the project definition of a throw. And thus the arbitrary enforcement of 22.5 degrees was implemented.

\subsection{Robot Path Planning}
As the robot is limited in both Cartesian and joint space, planning the path in either space leads to an unknown degree of complications in the other. As the joint restriction are generally harsher than the Cartesian boundaries, it was natural to do the path planning in joint space. This allows for full control of the joints positions at all times, however, it also limits the ability to generate specific movements in Cartesian space. The most important task of the path planning is to ensure that the TCP hits the release point in Cartesian space while moving in the direction of the ball's calculation trajectory with the correct velocity. Splitting the path planning in two sections makes this task much easier to achieve, as it is now possible to define the release point as either a start of end position in the trajectory, rather than some point along the path. With the path split in two, each section can now aim to achieve different goals.

The lead-up trajectory's goal is to hit the release point with the correct speed, this must be done while staying within joint limits. Calculating this part in joint space means it is possible to limit the range of motion needed for each joint based on its acceleration. However, that also means the correct velocity and direction in Cartesian space can only be ensured at the exact point of release. Any point before that may have any random direction and velocity at the TCP.

Since some error margin in the release timing must be expected, i.e. it might not be possible to have the ball lose contact with the gripper in that exact moment in time, it seemed optimal to have the TCP move in the direction of the ball trajectory for some amount of time. This becomes the goal of the follow-through section of the path. Determining this sequence in Cartesian space, allows the calculations to enforce a linear Cartesian direction of the TCP. During this movement, the robot also decelerates, which achieves two things. Firstly, the robot needs to slow down before hitting its physical limits and safety restriction. Secondly, the deceleration may counter the error margin in release timing. If the ball is released slightly too late, it will still be traveling in the correct direction towards the target, and though it is released closer to the target than expected, it is also released with a lower velocity. 

\subsection{Robot-table Calibration}
The initial calibration produced RMS errors ranging from 0.1991 m to 1.3078 m, which is significantly higher than expected for a robotâ€“table calibration. These large errors indicate that the estimated transformation was strongly affected by inconsistent data. Inspection of the 3D calibration plot showed that several points deviated clearly from the overall spatial structure and did not follow the expected geometry of the calibration surface, identifying them as outliers.

Such outliers are likely caused by inaccuracies in manual TCP placement, operator-induced measurement errors, or small misalignments between the calibration tool and the table surface. Since the calibration relied on manual positioning, even minor TCP deviations could result in substantial positional errors in the recorded data.

To improve robustness, an outlier detection procedure was applied by evaluating the Euclidean distance between corresponding robot-frame and world-frame points. Four points with disproportionately large errors were identified and removed to prevent bias in the SVD-based least-squares estimation of the rigid-body transformation.

After removing the outliers, the calibration was repeated, resulting in RMS errors between 0.0005 m and 0.0012 m. This significant reduction demonstrates the strong impact of outliers on transformation estimation and confirms the effectiveness of data filtering. The final transformation is therefore considered substantially more reliable and suitable for applications requiring accurate spatial alignment, highlighting the importance of careful data acquisition and validation in manually performed calibration procedures.

\subsection{Matlab communication}
In the final program Matlab Engine was used, but other options were explored. Alternatives include but are not limited to Matlab MEX functions and Comma-Separated Values file communication (CSV). CSV is the crudest version of this communication, as it required Matlab to already be running and checking the CSV file for changes. The code did work and made changes on one side very easy, but having two programs running defeated the point of integrating Matlab to have one seamless system, so it was decided to move away from CSV. Some of the code was repurposed for the storing of data, as mentioned in\ref(chap:datacollection).

The other options explored are native to Matlab. MEX is a way of recompiling a Matlab script into something that can be directly integrated into, among others, C++. It works by compiling both the script and the C++ code in Matlab and running it in that environment. That means that data can be passed back and forth directly in memory and would in theory be faster than Matlab engine. Downsides do however include limitations to toolboxes that were needed, and the fact that running it in Matlab could introduce more problems with communication with the robot and camera. 

Matlab engine works by starting the script as a process in the background, passing the data by value through the API, running it through the script, and returning it by value through the API to the C++ code. This is in theory slower, and with more data than needed for this project, it could be an issue having multiple copies of the same data. However, using it allowed running the C++ code natively instead of in Matlab, was easier to compile with libraries, allowed the use of toolboxes like the Robotics System Toolbox, all while still integrating Matlab directly with the C++ code, and therefore Matlab engine was the preferred option in this project.

\section{Results}
The results from the testing phase gives an indication as to how each parameter impacts the system. Firstly, the testing was done by simulating throws using varied parameters to find a suitable setup for the real-world testing.
 
\subsection{Weighted Jacobian}
Using a weighted Jacobian forces some joints to be more active. The implementation of this was done to limit the use of Wrist 2, which is highly restricted by the safety limits. This courses an increased usage of the other joints which resulted in the joint limits being exceeded more often during the lead-up path. However, it was determined that the increase in non-clipped follow-through paths out-ways the negative impact during lead-up, and thus the weighted Jacobian was used for all further testing.

\subsection{Acceleration}
It is natural that the robot cannot accelerate at an infinite rate, but finding which acceleration is possible with limited error margins proved to be difficult. Testing the impact of acceleration on the kinematic model was the first step to find a suitable level. It was found that an acceleration of $5 rad/s^2$ seemed to be a good middle ground, as it allows for hits at almost all tested target points. Increasing the acceleration by 20\% to $6 rad/s^2$ only added one additional reachable target. Keeping the acceleration relatively low is desirable as to keep the robot form having to do violent movements. 

As the robot path planning uses a static acceleration for the lead-up sequence, it is imperative that the robot is able to hit this acceleration profile as well. The movement control of the robot during the throw sequence is done by providing target speeds to the joints at each time step. The acceleration allowed to hit those targets must also be specified. Naturally, an acceleration lower than that of the model means it is impossible for the robot to hit the target speed at any point. Additionally, using an acceleration much higher than the model may lead to the robot stuttering as it will only need to accelerate at the start of each time step. A balance was found at a robot acceleration 25\% higher than the model, this provided the lowest mean absolute velocity error, meaning the robot's actual velocity was the closest to the model's.

\subsection{Release Timing}
It was quickly noted that there is a delay between calling the gripper to release the ball, and the ball physically being free from the robot. Determining this delay precisely was nearly impossible without any sensor feedback showing when contact with the ball was lost. Two approaches was used to estimate the delay, firstly, the gripper was filmed in slow motion at 480 frames per second, and secondly, testing was done using varied offsets. Using the slow motion video it was estimated that the delay was 40-50ms which was backed by the testing which showed the best results at an offset of 48ms. 

\subsection{Velocity Profile}
The method of splitting the path in two sections leads to a velocity profile in the shape of a triangle as seen in figure \ref{fig:VelocityOverTime}. In theory the path could have been split in a much larger number of sections, which would allow for more complex velocity profiles, but would also increase the complexity of the kinematics and mathematics needed substantially. The theorized benefit of using more sections is to allow for a differentiated acceleration throughout the sequence. This could help minimize the effects of jerk experienced as each joint starts moving. Had the time frame allowed, it would have been interesting to further test this hypothesis and try different profiles. 
